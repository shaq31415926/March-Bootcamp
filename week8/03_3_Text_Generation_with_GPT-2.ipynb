{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c75fd9-75c4-4792-bf17-672a14193fef",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64aae2a-ac3c-4a59-8fbc-b680d9c1c8a3",
   "metadata": {},
   "source": [
    "**Objective:** To generate creative text based on a given prompt using GPT-2.\n",
    "\n",
    "We can use the Hugging Face's Transformers library, which is an open-source library for NLP tasks. They offer pre-trained models like GPT-2, which is a smaller and more accessible version of GPT-3. \n",
    "\n",
    "For this activity, you will see how a generative model can continue a given text in a meaningful way\n",
    "\n",
    "**Instructions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c3c7-204f-4853-bbcc-ba6ed10ba568",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe545-d6c8-4b05-8ac8-39310accdce0",
   "metadata": {},
   "source": [
    "- You may need to install the necessary libraries if you haven't already, You can install them via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579a39f6-5b2f-466a-a045-b2e3d4b7654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install torch #Requirement library for some functionalities of transformers library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcabe5-9eb5-46ee-921f-036f15cccc13",
   "metadata": {},
   "source": [
    "- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad52d880-e212-4e91-8d67-d78dd5a4c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd85f8-ccde-4a61-8eda-4379cef5624b",
   "metadata": {},
   "source": [
    "- We import `torch` which is the library for PyTorch, a popular framework for deep learning.\n",
    "- From `transformers`, we import `GPT2Tokenizer` and `GPT2LMHeadModel`. The tokenizer will help us convert text to numbers that the model can understand, and GPT2LMHeadModel is the actual GPT-2 model we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115504d7-d96a-4c5e-8ffd-c694a245b98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load the pre-trained GPT-2 model and tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470a24a-e20f-4333-9c61-931274376ecb",
   "metadata": {},
   "source": [
    "We load the pre-trained GPT-2 model and tokenizer using the from_pretrained method, specifying 'gpt2' as the model we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fa7157-e80e-4d1a-b57a-30a15430c3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd4d47ba88f48de8ca4c292269495e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Utilisateur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e9bbdd50354d45b3a5ee75dc847c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2180e7d665e74f8bae5b81443efd23c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5060ec6b050043f9b21396a294cc80ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1afc65602f64a3ea30e6cc7f6db6c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1620421e651f48778255eccbf13d0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882789f8-666c-4bef-a351-de0b6fb94f19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create a function to generate text based on a given prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8480db4-03d0-439a-8468-258a5196e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    # Encoding the inputs\n",
    "    inputs = tokenizer.encode(prompt,return_tensors=\"pt\")\n",
    "    \n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create an attention mask\n",
    "\n",
    "    # Generating outputs using the encoded inputs with simplified parameters\n",
    "    outputs = model.generate(\n",
    "        inputs, # Make sure to pass the encoded inputs\n",
    "        attention_mask=attention_mask,  # Pass the attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set the pad token ID\n",
    "        max_length=150, # Output size\n",
    "        num_beams=5, # # Experiment with different values\n",
    "        temperature=0.9, # Lower temperature to make output more focused. Increase temperature for more randomness\n",
    "        do_sample=True,\n",
    "        top_k=50, # Lower top_k for more diversity\n",
    "        top_p=0.85, # Introduce top_p for nucleus sampling, for more diversity\n",
    "        no_repeat_ngram_size=2, # Prevent repeating n-grams of size 2. Increase to prevent repeating longer n-grams\n",
    "        early_stopping=True # Stop generating when conditions are met, to save time (in sometime Consider disabling)\n",
    "    )\n",
    "\n",
    "    # Decoding and printing the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'Generated Text:\\n{generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c16e7-a7cb-4df0-8f9b-9c309f79c0f3",
   "metadata": {},
   "source": [
    "The function `generate_text` is designed to generate a piece of text based on a given prompt using a pre-trained language model.\n",
    "let's break it down:\n",
    "\n",
    "- **Input Encoding:**\n",
    "\n",
    " - `inputs = tokenizer.encode(prompt, return_tensors=\"pt\")`: \n",
    "\n",
    "      - The `prompt` is processed (tokenized) to convert it into a numerical format that the machine learning model can understand.\n",
    "   - The result is a tensor, which is a multi-dimensional array used in machine learning tasks, specifically formatted for PyTorch (indicated by return_tensors=\"pt\").\n",
    "   \n",
    "- **Text Generation:**\n",
    "\n",
    "    - `outputs = model.generate(inputs, max_length=150, num_beams=5, temperature=1.5, top_k=50)`:\n",
    "               - The pre-trained model is instructed to generate text based on the provided inputs.\n",
    "             - Various parameters like `max_length`, `num_beams`, `temperature`, `top_k`, ect are set to control the text generation process, impacting the length, creativity, and quality of the generated text.\n",
    "             \n",
    "\n",
    "- **Output Decoding:**\n",
    "\n",
    "  - `generated_text = tokenizer.decode(outputs[0])`:\n",
    "         - The numerical output from the model is translated back into human-readable text using the tokenizer.\n",
    "        - Only the first output (`outputs[0]`) is decoded as the model is set up to generate one piece of text in this case.\n",
    "\n",
    "- **Display Generated Text:**\n",
    "\n",
    "- `print(f'Generated Text:\\n{generated_text}')`:\n",
    "        - Finally, the generated text is printed to the console, prefixed with the label \"Generated Text:\".\n",
    "\n",
    "In summary, this function encapsulates the process of taking a textual prompt, processing it for the model, generating new text based on that prompt, decoding the generated text back into a human-readable form, and then displaying the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567dc0c-f3b5-4977-8ef5-33d712f5858d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### About Text Generation Parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b8ce4-44f4-49ae-97c3-4bb2094927c4",
   "metadata": {},
   "source": [
    "Below are the parameters used in the model.generate method within the generate_text function:\n",
    "\n",
    "**max_length=150:**\n",
    "\n",
    "- This parameter sets the maximum number of tokens in the generated text. If the model reaches this length, it will stop generating further tokens.\n",
    "\n",
    "**num_beams=5:**\n",
    "\n",
    "- Beam search is a heuristic search algorithm used in machine learning. The `num_beams` parameter specifies the number of beams (or hypotheses) to maintain when generating text. A higher number of beams can result in better quality output, but at the cost of computational resources.\n",
    "\n",
    "**temperature=0.7:**\n",
    "\n",
    "- The `temperature` parameter helps control the randomness of the output. Lower values (like 0.7) make the output more focused and deterministic, while higher values make the output more random and creative.\n",
    "\n",
    "**top_k=50:**\n",
    "\n",
    "- During text generation, the `top_k` parameter restricts the selection pool for the next token to the top K probable tokens. This helps in reducing the chance of getting unlikely or rare tokens and keeps the generation process on track.\n",
    "\n",
    "**no_repeat_ngram_size=2:**\n",
    "\n",
    "- This parameter helps prevent the model from generating repeating n-grams (a sequence of n words) of size 2. This can aid in reducing repetitiveness in the generated text.\n",
    "\n",
    "**early_stopping=True:**\n",
    "\n",
    "- The `early_stopping` parameter is a boolean flag that, when set to `True`, stops the text generation process once certain conditions are met (like reaching an end-of-sequence token), helping to save time and computational resources.\n",
    "\n",
    "These parameters are used to control and fine-tune the text generation process, making it easier to obtain desirable and coherent text based on a given prompt.\n",
    "\n",
    "- **Note:**\n",
    "There are several other parameters we can include to control the text generation process using the `model.generate` method. The parameters and their descriptions can be found in the documentation for the specific library we are using in our case it is Hugging Face Transformers. Check out the following links:\n",
    "- [Hugging Face, OpenAI GPT2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html)\n",
    "- [Hugging Face, Model](https://huggingface.co/transformers/v2.11.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85650d24-a998-4bf7-8b18-2167f1f4d532",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Applying the generate_text function in out text Generated Text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ada26c-4085-4d13-834d-f5228b2cdcc6",
   "metadata": {},
   "source": [
    "Now, call the generate_text function with a creative prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1f74fe-d5ce-4689-9022-d66ee853e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, in a land far far away, there was a man who had been born and raised in the land of his birth.\n",
      "\n",
      "He was the son of a nobleman, and he was born into a family of noblemen. He was raised to be a knight, but he did not know how to become one. In his youth, he had not been able to learn the art of war, nor did he know the arts of fighting. But when he came of age, his father said to him, \"You are a good boy. You have been raised by your father and your mother and you are ready to fight for the cause of your country.\" And he fought. And so he became a hero of the people\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d06aa-3b68-4ee2-9e16-e0239ceaf5ff",
   "metadata": {},
   "source": [
    "## Activity Extension & Discussion: Enhancing Creativity in Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476804ca-8b85-4a35-af4e-1e03e2e8086b",
   "metadata": {},
   "source": [
    "Now that we have had our hands-on experience with basic text generation using GPT-2, let's dive a bit deeper to explore how we can tune the model to generate more creative text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19791f7-8ff2-4b97-aa3b-8edd8708e640",
   "metadata": {},
   "source": [
    "#### 1. Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f1ebe-7513-40be-a1a4-7003d2b94649",
   "metadata": {},
   "source": [
    "- **Temperature:**\n",
    "   - Adjusting the temperature parameter influences the randomness of the output.\n",
    "    - Higher values (e.g., closer to 1 or above) yield more creative and random outputs, whereas lower values make the output more focused and deterministic.\n",
    "- **Top_k:**\n",
    "    - The top_k parameter restricts the model to choose the next word from the top k probable words.\n",
    "     - Lower values of top_k can introduce more diversity in the generated text.\n",
    "- **Top_p (Nucleus Sampling):**\n",
    "     - The top_p parameter allows for a more dynamic truncation of the vocabulary during sampling.\n",
    "      - By introducing top_p, you're allowing the model to consider a varying set of most probable words to choose the next word from, which can lead to more creative text.\n",
    "- **Num_beams (Beam Search):**\n",
    "    - The num_beams parameter affects the beam search process, which tends to focus the output towards the most probable sequences.\n",
    "     - Adjusting or removing num_beams can increase creativity by letting the model explore a wider range of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e45b2c-7c87-46fe-87e7-2d0192921784",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcd1cc-d612-4b50-a397-0496e95b44fa",
   "metadata": {},
   "source": [
    "Update the generate_text function with new parameter values based on the above discussion.\n",
    "Compare the text generated with different parameter settings and discuss the observed changes in creativity and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5ab860-8ed8-455b-80c5-a482afb21914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Once upon a time, in a land far far away, there was a man who had no fear of death, and who was not afraid of his life.\n",
      "\n",
      "And now, when he was in the midst of this, he said to the man, \"Why do you fear death?\" The man answered: \"I am afraid that my life will be so short as to give way to fear. For I know that I will die in this life; I do not know how it will end; but it is possible that it may end in death. And I have no desire to die, for I want to live in peace.\" And he went on to tell of the life that he would have had if he had never known that death was the end of all things. But that man did not die; he died by the hand of God and by his own will, because God had given to him a life of peace, which he could not have lived without. Therefore, I ask you, why did you give up your life for the sake of fear? And what was it that you did for your own sake? For you have not believed that God would allow you to go on living, so that if you die you may go to a better place. Do you not believe in God's will and the will of man? If you do, you will see that the Lord has given you life, but he has not given it to you. So, if I had been able to\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(prompt,return_tensors=\"pt\")\n",
    "    \n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)  # Create an attention mask\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs, # Make sure to pass the encoded inputs\n",
    "        attention_mask=attention_mask,  # Pass the attention mask\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set the pad token ID\n",
    "        max_length=300, # Output size\n",
    "        num_beams=5, # Experiment with different values\n",
    "        temperature=1.5,  # Lower temperature to make output more focused. Increase temperature for more randomness\n",
    "        top_k=30,  # Lower top_k for more diversity\n",
    "        top_p=0.85,  # Introduce top_p for nucleus sampling, for more diversity\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating n-grams of size 2. Increase to prevent repeating longer n-grams\n",
    "        early_stopping=True,  # Stop generating when conditions are met, to save time (in sometime Consider disabling)\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0])\n",
    "    print(f'Generated Text:\\n{generated_text}')\n",
    "\n",
    "# Example usage\n",
    "generate_text(\"Once upon a time, in a land far far away,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b786fe-d92e-47f9-b3da-aea59f1d4798",
   "metadata": {},
   "source": [
    "#### 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f7cfa-8ecd-4ed3-9246-da487157bd11",
   "metadata": {},
   "source": [
    "- How did the different parameter settings impact the creativity and coherence of the generated text?\n",
    "- Were there certain settings that produced more desirable or interesting results?\n",
    "- How might these parameter tweaks be useful in different text generation applications?\n",
    "\n",
    "This extension aims to provide a more nuanced understanding of how various parameters affect text generation with GPT-2, paving the way for attendees to experiment and discover optimal settings for their own use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e71789-9e3f-49d4-9e08-25c56cf725c2",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1e320-15e7-4b89-b5ce-3788f0faf6cb",
   "metadata": {},
   "source": [
    "- **Control**: Although we can influence the generated text with various parameters, achieving precise control over the content, style, or tone remains a challenge.\n",
    "- **Context Understanding**: The model might sometimes generate text that's contextually irrelevant or incorrect, as it solely relies on patterns learned from the training data without understanding the content.\n",
    "- **Lengthy Text**: Generating lengthy coherent text is still a challenge as the coherence often dwindles as the text gets longer.\n",
    "- **Computational Resources**: Generating text with large models like GPT-2 requires substantial computational resources, which might be a limitation for real-time applications or individuals with limited computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
