{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce092749-3c51-4c59-bebe-b2eb0cb9c1b0",
   "metadata": {},
   "source": [
    "# Text Processing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51978819-15a6-46ce-b47b-9e30de1eb152",
   "metadata": {},
   "source": [
    "- **Introduction:**\n",
    "\n",
    "This script showcases a standard text processing pipeline using the SpaCy library. SpaCy is a modern and fast NLP library that allows you to perform a variety of text analysis tasks. From tokenization to Named Entity Recognition (NER), SpaCy provides easy-to-use tools and pre-trained models to help you process text data efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43b623-5478-48fb-aa16-ba1d7d3bfe8a",
   "metadata": {},
   "source": [
    "- **Objective:**\n",
    "\n",
    "The primary objective of this code is to demonstrate various text preprocessing steps including tokenization, stop word removal, lemmatization, part-of-speech tagging, named entity recognition, and additional preprocessing such as removing punctuation and numbers. These steps are crucial in preparing text data for downstream tasks like text classification, sentiment analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8055f-258f-4ed5-87c9-317641d1394a",
   "metadata": {},
   "source": [
    "## 1. Importing the necessary library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e5770-54c9-4a7d-8990-9045461e8e19",
   "metadata": {},
   "source": [
    "First, ensure you have SpaCy installed along with the language model. \n",
    "\n",
    "You can install SpaCy and download the language model using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b26d45-7030-4e6a-99a6-aa55aae95a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6acad7f-5f0b-47c8-b1e8-124dd1617f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efd0d4-3b3c-47e8-af72-4eba2dbb98e4",
   "metadata": {},
   "source": [
    "- `spacy` is a library for advanced Natural Language Processing in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52dc3d-9415-4958-a7c3-31d7662fee13",
   "metadata": {},
   "source": [
    "## 2. Loading a SpaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f450c3-709d-4f0b-ac77-d491e0617cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab61bf6-d977-4db0-afe2-18d6785d2197",
   "metadata": {},
   "source": [
    "`spacy.load('en_core_web_sm')` loads a pre-trained statistical model for English. This model supports identification of token boundaries (tokenization) and other linguistic annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f737b1-db6d-49c0-a631-af8da20477ac",
   "metadata": {},
   "source": [
    "## 2. Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2305847-cce1-4bb7-81f3-357a3faf7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79101e7-220b-495a-a4b6-b25eee427ba6",
   "metadata": {},
   "source": [
    "This is the text we want to process using SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5970a8-e2a6-4a20-86b4-9982dafe715a",
   "metadata": {},
   "source": [
    "## 3. Processing text with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e76061-844a-4e56-a898-3d6c7a34712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0225085-40f2-4c28-a6a0-15dd944f4fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736c203-93f0-4a7a-864d-1b3405ab389b",
   "metadata": {},
   "source": [
    "Here, `nlp` is applied to the text, which invokes the model on the text and produces a `Doc` object enriched with linguistic annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db46ba9-704c-42f1-b8cc-c104f0087ad9",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e59ade7-8e06-4ac7-9950-57e6ec883e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['Apple', 'Inc.', 'is', 'an', 'American', 'multinational', 'technology', 'company', 'headquartered', 'in', 'Cupertino', ',', 'California', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenization_example = [token.text for token in doc]\n",
    "print(\"Tokenization:\", tokenization_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4a24f-c915-4cde-bbf8-7aa5a767762d",
   "metadata": {},
   "source": [
    "- Tokenization breaks text into words, phrases, symbols, or other meaningful elements called tokens. In this code, we extract the text of each token from the `doc` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6b619-ebeb-4edc-87ba-e2708502a141",
   "metadata": {},
   "source": [
    "### 2. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822c24ac-542d-4a94-8688-8f8d8b13dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Word Removal: ['Apple', 'Inc.', 'American', 'multinational', 'technology', 'company', 'headquartered', 'Cupertino', ',', 'California', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_word_removal_example = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Stop Word Removal:\", stop_word_removal_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ceebc0-4686-4300-b5af-84f9deb8884d",
   "metadata": {},
   "source": [
    "- Get the list of stop words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb511c2e-4110-4722-8cbc-015a4a47782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quite', 'toward', 'myself', 'third', '’ve', 'none', 'full', 'have', 'some', 'more', 'out', 'therefore', 'become', 'beyond', 're', 'thereupon', '‘d', 'while', 'again', 'nobody', 'without', 'here', 'had', 'yourself', 'our', 'one', 'nothing', 'off', 'thereby', 'name', 'next', 'noone', 'its', 'nowhere', 'elsewhere', '’m', 'alone', 'himself', 'those', 'within', '’d', 'six', 'through', 'than', 'four', 'an', 'does', 'unless', 'anything', 'meanwhile', 'behind', 'ourselves', 'formerly', 'keep', 'over', 'due', 'anyhow', 'becomes', 'each', 'last', 'they', 'hers', 'make', 'from', 'several', 'much', 'whereas', 'go', 'still', '’ll', 'whose', 'together', 'ten', 'show', 'further', 'on', 'various', 'neither', 'except', '’re', 'against', 'ever', 'can', 'whether', 'the', 'and', 'beforehand', 'often', 'before', 'five', 'anyway', 'after', 'fifteen', 'other', 'former', 'therein', 'both', 'him', 'perhaps', 'another', 'who', 'has', 'latterly', \"'m\", 'twelve', 'whenever', 'wherever', 'a', 'really', 'via', 'part', 'seem', 'seems', 'many', 'hereafter', 'do', 'itself', 'could', 'thru', 'since', 'just', 'throughout', 'two', 'towards', 'around', 'of', 'same', \"'ve\", 'us', 'eleven', 'anyone', 'if', \"'ll\", 'becoming', 'not', 'only', 'well', 'doing', 'namely', 'at', 'amount', 'how', 'what', 'hence', 'there', 'whereafter', 'mine', 'your', 'also', 'call', 'see', 'get', 'about', 'most', 'her', 'no', 'with', 'whereby', 'forty', 'whence', 'between', 'rather', 'used', 'among', 'any', 'became', 'put', 'top', 'them', 'will', 'whatever', 'seemed', 'as', 'might', 'we', 'am', 'every', 'thereafter', 'bottom', 'yourselves', 'otherwise', 'been', 'must', 'latter', 'using', 'below', 'once', '‘m', 'serious', 'because', 'would', 'should', 'along', 'move', 'me', 'all', 'everyone', 'so', 'sixty', 'yet', 'onto', 'in', 'already', 'whither', 'afterwards', 'themselves', 'during', 'until', 'their', 'sometime', 'anywhere', 'n’t', 'regarding', 'indeed', 'give', \"n't\", 'now', 'herself', 'either', \"'s\", 'ca', \"'d\", 'beside', 'least', 'please', 'for', 'although', 'upon', 'nine', 'hereby', 'nevertheless', 'i', 'by', 'yours', 'hundred', 'was', 'where', '’s', 'very', 'down', '‘re', 'my', 'mostly', 'somewhere', 'whole', 'moreover', 'n‘t', 'is', 'everything', 'such', 'she', 'into', 'nor', 'why', '‘ll', \"'re\", 'it', 'everywhere', 'back', 'something', '‘s', 'less', 'three', 'cannot', 'per', 'someone', 'wherein', 'when', 'ours', 'others', 'never', 'this', 'almost', 'these', 'eight', 'herein', 'however', 'even', 'may', 'whom', 'say', 'his', 'too', 'empty', 'though', 'side', 'take', 'thence', 'whereupon', 'always', 'hereupon', 'seeming', 'he', 'across', 'you', 'besides', 'somehow', 'are', 'enough', 'up', 'few', 'whoever', 'front', 'but', 'else', 'twenty', 'amongst', '‘ve', 'did', 'fifty', 'own', 'done', 'first', 'were', 'under', 'which', 'that', 'above', 'or', 'then', 'to', 'thus', 'being', 'made', 'be', 'sometimes']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stop words\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# If you want to print the stop words, you can convert the set to a list and print it\n",
    "print(list(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906fb8a-005e-498a-b01b-d54b7f40403f",
   "metadata": {},
   "source": [
    "- Stop words are common words that are often removed in the preprocessing step. `token.is_stop` checks if a token is a stop word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc2d3d-1c43-4f7d-a69f-1221c2f5b8a8",
   "metadata": {},
   "source": [
    "### 3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3637e879-c4bf-47a7-b61b-22798a5675d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['Apple', 'Inc.', 'be', 'an', 'american', 'multinational', 'technology', 'company', 'headquarter', 'in', 'Cupertino', ',', 'California', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatization_example = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatization:\", lemmatization_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6458d94b-c6f4-43c3-b172-5f8174025655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['Apple', 'Inc.', 'be', 'an', 'american', 'multinational', 'technology', 'company', 'headquarter', 'in', 'Cupertino', ',', 'California', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatization_example = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatization:\", lemmatization_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271daed3-c37a-47ad-8d0e-3eeb3bd76c59",
   "metadata": {},
   "source": [
    "- Lemmatization reduces words to their base or root form. Here, `token.lemma_`gets the lemma of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1479965-ea51-49d3-891a-faeaf72a0054",
   "metadata": {},
   "source": [
    "**Note:** SpaCy offers lemmatization instead of stemming because it considers lemmatization to be more accurate and reasonable for understanding words in context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145233d-fccf-44d2-ba15-13a2ca03848e",
   "metadata": {},
   "source": [
    "### 4. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f358eb34-03e4-41b1-b7fe-f901ef32ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-Speech Tagging: [('Apple', 'PROPN'), ('Inc.', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('American', 'ADJ'), ('multinational', 'ADJ'), ('technology', 'NOUN'), ('company', 'NOUN'), ('headquartered', 'VERB'), ('in', 'ADP'), ('Cupertino', 'PROPN'), (',', 'PUNCT'), ('California', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagging_example = [(token.text, token.pos_) for token in doc]\n",
    "print(\"Part-of-Speech Tagging:\", pos_tagging_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49521bb-6b3e-4819-b3bc-6a298e713761",
   "metadata": {},
   "source": [
    "- POS tagging assigns a part of speech label to each token (e.g., noun, verb, adjective). `token.pos_` gets the POS tag of each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54b383-69d3-4b7f-a17b-2b56df20ff4d",
   "metadata": {},
   "source": [
    "### 5. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3df4c5-1916-485a-aebd-b438bd3eeaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition: [('Apple Inc.', 'ORG'), ('American', 'NORP'), ('Cupertino', 'GPE'), ('California', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "ner_example = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Named Entity Recognition:\", ner_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e41f3c-8b7f-4f5b-99bd-f75870b9fcca",
   "metadata": {},
   "source": [
    "- NER identifies and categorizes entities within the text (e.g., person names, organizations, locations). `doc.ents` retrieves the named entities, and `ent.label_` gets the label of each entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfbe95-dcfa-4cad-addd-57d817586cbc",
   "metadata": {},
   "source": [
    "### 6. Additional Preprocessing - Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd68b77d-9de4-42e8-94b2-39118c3e06cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Preprocessing: ['Apple', 'is', 'an', 'American', 'multinational', 'technology', 'company', 'headquartered', 'in', 'Cupertino', 'California']\n"
     ]
    }
   ],
   "source": [
    "additional_preprocessing_example = [token.text for token in doc if token.is_alpha]\n",
    "print(\"Additional Preprocessing:\", additional_preprocessing_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5035930-e536-4246-b806-bd6660d4fdc2",
   "metadata": {},
   "source": [
    "- This step filters out tokens that are not purely alphabetic. `token.is_alpha` checks if a token consists of alphabetic characters only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e380f-36c2-4d35-b0da-6cff5cab3e2d",
   "metadata": {},
   "source": [
    "### Performing all preprocessing steps at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6926b848-95da-44d6-8154-ef6412739fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple american multinational technology company headquarter Cupertino California\n"
     ]
    }
   ],
   "source": [
    "# Performing all preprocessing steps at once\n",
    "cleaned_text = ' '.join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
    "\n",
    "# Displaying the cleaned text\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4ff13-ffe7-429e-863d-5cc90f6afcd3",
   "metadata": {},
   "source": [
    "Here, in a single line of code, we achieve stop word removal, lemmatization, and additional preprocessing to exclude punctuation and numbers. This is done by iterating through each token in the document, verifying that it's not a stop word and that it's an alphabetic token, before extracting the lemma of each token. The resulting lemmatized tokens are then concatenated with spaces to form the cleaned-up text, which is subsequently displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c1152-1d7d-4979-9e51-0304574ac855",
   "metadata": {},
   "source": [
    "These steps provide a good preprocessing pipeline that prepares text data for further analysis or machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
