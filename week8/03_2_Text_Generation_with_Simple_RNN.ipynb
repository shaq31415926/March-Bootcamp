{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeae0a3-9bda-49c6-817f-e23052c3464f",
   "metadata": {},
   "source": [
    "# Text Generation with Simple Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6986aa-b92d-49a7-bdb3-3c7786d6f791",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "    \n",
    "In this activity, we'll explore basic text generation using a simple RNN on a small dataset containing the phrase \"hello world\". \n",
    "\n",
    "RNNs are suitable for sequence data, making them useful for tasks like text generation.\n",
    "\n",
    "**Objective:**\n",
    "We aim to train the RNN to understand the structure of the phrase so that it can generate text resembling it, given an initial character.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Prepare the data: Tokenize the text into characters and organize it into input-output pairs.\n",
    "2. Define the RNN model: Utilize TensorFlow to create a simple RNN model.\n",
    "3. Train the model: Train the RNN on the data to learn the text structure.\n",
    "4. Generate text: Use the trained model to generate new text based on a given seed character.\n",
    "5. Discussing the key challenges encountered\n",
    "\n",
    "This activity will provide a hands-on understanding of basic text generation concepts using RNNs and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f494e-908b-439b-bc81-34b44ed33ce3",
   "metadata": {},
   "source": [
    "### Install the necessary library using pip:\n",
    "\n",
    "First, you would need to install TensorFlow if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc194a05-4a95-4cdb-91ea-e0c3e6e0d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408c38d-cb17-4332-b92c-91cc65936502",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa9e6b8-d42c-4486-b213-03f75ea16cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dcf1d1-34c4-4135-baa9-ca357ea5ef62",
   "metadata": {},
   "source": [
    "The necessary libraries and modules from TensorFlow are imported for processing the data and building the RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9c205-f1cd-4e84-bee5-1ee2c8695125",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f065ab4c-d721-47ae-a69a-bab38e888612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tiny dataset\n",
    "data = \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf900b3-e865-4d96-b28a-9460495fdab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b78a24-e67e-4904-a0b4-972117a1daac",
   "metadata": {},
   "source": [
    "A minimal dataset \"hello world\" is defined. This dataset is what the model will learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0600b6-db04-4e4d-8b4c-bcc07f77fcfb",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579b786-e215-4141-9b7e-6b7745e56b00",
   "metadata": {},
   "source": [
    "The text data is converted into a numerical format using a character-level tokenizer. Each unique character gets mapped to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5fb343-fb0a-4aff-b354-9140f950ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)  # char_level=True for character tokenization\n",
    "tokenizer.fit_on_texts([data])\n",
    "sequences = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e82fc-4943-4447-bea7-197edcc7cc06",
   "metadata": {},
   "source": [
    "In this block:\n",
    "\n",
    "- A Tokenizer object is created with char_level=True to tokenize the text at the character level.\n",
    "- `fit_on_texts()` method is called to fit the tokenizer on the data.\n",
    "- `texts_to_sequences()` method is used to convert the text to a sequence of integers, where each integer represents a unique character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d917805a-93d9-4aa4-8533-4cfd217fdd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x0000026A5C8FEBB0>\n",
      "[3, 4, 1, 1, 2, 5, 6, 2, 7, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e08b3-1497-4295-94ca-9063a8bdd58e",
   "metadata": {},
   "source": [
    "## 3. Preparing Input and Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571027f-ea69-423f-a08b-627f94054dc2",
   "metadata": {},
   "source": [
    "Sequences of characters are prepared as input for the model, with corresponding next characters as output. This teaches the model the sequence in which characters appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6904d596-0bcf-4d50-8ce3-3d63d5dde44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "for i in range(1, len(sequences)):\n",
    "    input_sequences.append(sequences[:i])\n",
    "    output_sequences.append(sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ee8c11-ceac-4920-8f19-96c405d2ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequences: [[3], [3, 4], [3, 4, 1], [3, 4, 1, 1], [3, 4, 1, 1, 2], [3, 4, 1, 1, 2, 5], [3, 4, 1, 1, 2, 5, 6], [3, 4, 1, 1, 2, 5, 6, 2], [3, 4, 1, 1, 2, 5, 6, 2, 7], [3, 4, 1, 1, 2, 5, 6, 2, 7, 1]]\n",
      "output_sequences: [4, 1, 1, 2, 5, 6, 2, 7, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "print('input_sequences:', input_sequences)\n",
    "print('output_sequences:', output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06b30e1-4d86-4d89-bf7d-542171e47936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fd5f2-ae27-438b-87b1-a223fa361438",
   "metadata": {},
   "source": [
    "Here:\n",
    "\n",
    "- Two empty lists input_sequences and output_sequences are created to hold - the input and output data for the model.\n",
    "- A loop iterates through the sequence of integers, creating sub-sequences. Each sub-sequence (excluding the last character) is added to `input_sequences`, and the corresponding next character is added to `output_sequences`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963bcc7-f786-4f0c-bd77-f4a764730f4e",
   "metadata": {},
   "source": [
    "## 4. Padding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef8f54-bc55-4b4a-bb20-dada5147a862",
   "metadata": {},
   "source": [
    "All input sequences are padded to have the same length to ensure consistent input shape for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87901aa3-e610-4787-bed3-2b52ed3a1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = pad_sequences(input_sequences)\n",
    "output_sequences = tf.keras.utils.to_categorical(output_sequences, num_classes=len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00dbc2a-4b85-40a8-9cf2-cb9a06c9f1f3",
   "metadata": {},
   "source": [
    "- `pad_sequences()` is used to ensure that all sequences in the same size.\n",
    "- `input_sequences` have the same length by padding them with zeros.\n",
    "- `output_sequences` are one-hot encoded using to_categorical() to prepare them for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0054adf0-a5d4-40bd-aceb-50c9dacdb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequences: [[0 0 0 0 0 0 0 0 0 3]\n",
      " [0 0 0 0 0 0 0 0 3 4]\n",
      " [0 0 0 0 0 0 0 3 4 1]\n",
      " [0 0 0 0 0 0 3 4 1 1]\n",
      " [0 0 0 0 0 3 4 1 1 2]\n",
      " [0 0 0 0 3 4 1 1 2 5]\n",
      " [0 0 0 3 4 1 1 2 5 6]\n",
      " [0 0 3 4 1 1 2 5 6 2]\n",
      " [0 3 4 1 1 2 5 6 2 7]\n",
      " [3 4 1 1 2 5 6 2 7 1]]\n"
     ]
    }
   ],
   "source": [
    "print('input_sequences:', input_sequences)\n",
    "#print('output_sequences:', output_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0fda7e-7301-4b66-8796-641a16b73e5d",
   "metadata": {},
   "source": [
    "## 5. Split Data into Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b52f9-120d-4bd1-a6fb-55e83871149d",
   "metadata": {},
   "source": [
    "Data is organized into features (X) and targets (y), where X is the input to the model and y is the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9428f6d2-1c0f-4b20-b48c-56659b592ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = input_sequences, output_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5a510-a99c-4f7a-a311-dbc0ba296c79",
   "metadata": {},
   "source": [
    "The input and output data are assigned to X and y respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d32851-e06d-4887-8c8c-4bd49cf378ba",
   "metadata": {},
   "source": [
    "## 6. Defining the RNN Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54621626-e236-4ffb-8296-f2a1d05f1bce",
   "metadata": {},
   "source": [
    "A simple RNN model is defined using:\n",
    "- an embedding layer (to handle the input data), \n",
    "- a SimpleRNN layer (the recurrent part of the network), \n",
    "- and a dense layer (for final output predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "876392bf-f047-445c-960d-6269824e383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # input layer\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8, input_length=input_sequences.shape[1]),\n",
    "    # RNN layer\n",
    "    tf.keras.layers.SimpleRNN(8),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bc7d2-c8c6-4164-b694-3592f944e97e",
   "metadata": {},
   "source": [
    "In this block:\n",
    "\n",
    "- A sequential model is defined using the Sequential() class.\n",
    "- An Embedding layer is added to learn a dense representation of the input sequences.\n",
    "- A SimpleRNN layer with 8 units is added.\n",
    "- A Dense layer with a softmax activation is added for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bc50f-31ed-4698-91e6-27e1a347cfba",
   "metadata": {},
   "source": [
    "## 7. Compiling and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd5b9f62-8f33-434b-b403-2bfc67678e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a5cb5bbe0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640b62d-e6b7-4cb4-a4c0-53f5e7937361",
   "metadata": {},
   "source": [
    "- The model is compiled using a loss function, optimizer, and a metric to monitor during training. These settings are standard for a classification task.\n",
    "\n",
    "- The model is trained on the data for 50 epochs to learn the patterns in the input text and to predict the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd773a-578e-4f90-9668-40ec06b2decb",
   "metadata": {},
   "source": [
    "## 8. Defining a Function to Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdecf7c-2667-408e-9482-f9eb3c610262",
   "metadata": {},
   "source": [
    "A function to generate text is defined. Given a seed text, it predicts the next characters iteratively using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23903533-fee8-4e85-9c43-cd262e50d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_chars=10):\n",
    "    for _ in range(next_chars):\n",
    "        sequences = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        sequences = pad_sequences([sequences], maxlen=input_sequences.shape[1])\n",
    "        predicted = model.predict(sequences, verbose=0)\n",
    "        predicted_index = np.argmax(predicted, axis=-1)[0]\n",
    "        seed_text += tokenizer.index_word[predicted_index]  \n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c38e1c-b4e1-4511-ad3d-6721167c0170",
   "metadata": {},
   "source": [
    "In this block:\n",
    "\n",
    "- A function `generate_text` is defined to generate text given a seed text.\n",
    "- The function iterates for the specified number of characters (`next_chars`), tokenizes the current text, pads it, predicts the next character using the model, and appends the predicted character to the seed text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc59f6e-ad8a-4128-9df9-57a95d30df4d",
   "metadata": {},
   "source": [
    "## 9. Generating and Printing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c7ec6-bc7e-40fc-95d4-9c59ef459736",
   "metadata": {},
   "source": [
    "Finally, the generate_text function is used to generate text, starting with the character \"h\", and prints the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2747cd56-db12-4666-afd3-5ec05ac170f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello worldrdeellorll\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(\"h\", next_chars=20)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c614f-8f4b-463b-83c9-59171f6814ee",
   "metadata": {},
   "source": [
    "- The generate_text function is called with a seed text of \"h\" and a request to generate 10 additional characters.\n",
    "- The generated text is printed to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d60b0f-88e9-4db0-89ce-f16a4dc69f45",
   "metadata": {},
   "source": [
    "This structure illustrates a simplified approach to text generation with an RNN, from data preparation to model training and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21822f5-5773-4dca-8427-f425e5be3d89",
   "metadata": {},
   "source": [
    "## 10. Key challenges encountered in the simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1d6ba-3558-4bec-a188-6f207454b0df",
   "metadata": {},
   "source": [
    "The simple example provided serves as an introductory exercise to text generation using RNNs, but it has several limitations and challenges, including:\n",
    "\n",
    "**Small Dataset:**\n",
    "\n",
    "- The dataset used is extremely small (\"hello world\"), which does not allow the model to learn a rich representation of language.\n",
    "- A small dataset may lead to overfitting where the model memorizes the data rather than learning the underlying patterns.\n",
    "\n",
    "**Simplicity of Model:**\n",
    "\n",
    "- A Simple RNN is used, which is quite basic and may struggle with complex text generation tasks.\n",
    "\n",
    "**Character-Level Tokenization:**\n",
    "\n",
    "- The model operates on individual characters, which can be limiting compared to understanding whole words or phrases.\n",
    "\n",
    "**Lack of Generalization:**\n",
    "\n",
    "- The setup is quite simplistic and may not perform well on different or more complex text data.\n",
    "\n",
    "**Text Generation Quality:**\n",
    "\n",
    "- The quality of the generated text might not meet expectations, especially with varying input or on a larger scale.\n",
    "\n",
    "These challenges highlight the need for a more sophisticated approach, larger datasets, and potentially more advanced models to achieve better text generation results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
